#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Video people-tracking + congestion, line-cross counting, occupancy overlay
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
* Congestion   : 0-100 % = (min(occupancy / max_people, 1) Ã— 100)
* People count : forward / backward crossings over a user-defined line
* Occupancy    : current # of tracked persons in frame
* Heat-map     : cumulative person locations, shown as minimap (top-right)
* Colours      : Supervision TRACK palette (same as masks / labels)
* Requirements : supervision â‰¥ 0.21, ultralytics (YOLOE fork) + BoT-SORT
"""
import argparse
import os
import random
from collections import defaultdict

import cv2
import numpy as np
from PIL import Image
from tqdm import tqdm
import supervision as sv
from ultralytics import YOLOE
from ultralytics.models.yolo.yoloe.predict_vp import YOLOEVPSegPredictor
import torch

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
from tracker.boostTrack.GBI import GBInterpolation
from tracker.boostTrack.boost_track import BoostTrack


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ObjectMeta Class â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
class ObjectMeta:
    """
    ê°œì²´ì˜ ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ ë‹´ëŠ” í´ë˜ìŠ¤
    Detectionë¶€í„° Tracking, Overlayê¹Œì§€ ì¼ê´€ë˜ê²Œ ì‚¬ìš©
    """
    def __init__(self, box=None, mask=None, confidence=None, class_id=None, 
                 class_name=None, track_id=None):
        self.box = box              # [x1, y1, x2, y2] or [cx, cy, w, h]
        self.mask = mask            # segmentation mask array
        self.confidence = confidence # detection confidence score
        self.class_id = class_id    # class index
        self.class_name = class_name # class name string
        self.track_id = track_id    # tracking ID (None initially)
    
    def __repr__(self):
        return f"ObjectMeta(class={self.class_name}, track_id={self.track_id}, conf={self.confidence:.2f})"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    # I/O
    parser.add_argument("--source", type=str, default="/DL_data_super_hdd/video_label_sandbox/efg_cargil2025_test1.mp4",
                        help="Input video path")
    parser.add_argument("--output", type=str, default="output",
                        help="Output directory (optional, defaults to input filename without extension)")
    # Logging
    parser.add_argument("--log-detections", type=bool, default=True,
                        help="Log detection results to label.txt")
    # Model
    parser.add_argument("--checkpoint", type=str,
                        default="/works/samsung_prj/pretrain/yoloe-11l-seg.pt",
                        help="YOLOE checkpoint (detection + seg)")
    parser.add_argument("--names", nargs="+",
                        default=["fish", "disco ball", "pig"],
                        help="Custom class names list (index order matters)")
    parser.add_argument("--device", type=str, default="cuda:0",
                        help="Inference device")
    # Inference / tracking
    parser.add_argument("--image-size", type=int, default=640,
                        help="Inference image size")
    parser.add_argument("--conf-thresh", type=float, default=0.1,
                        help="Detection confidence threshold")
    parser.add_argument("--vp-thresh", type=float, default=0.1,
                        help="visual prompt confidence threshold")
    parser.add_argument("--iou-thresh", type=float, default=0.45,
                        help="Detection NMS IoU threshold")
    parser.add_argument("--track-history", type=int, default=100,
                        help="Frames kept in trajectory history")
    parser.add_argument("--track-det-thresh", type=float, default=0.1,
                        help="Detection confidence threshold for tracking")
    parser.add_argument("--track-iou-thresh", type=float, default=0.3,
                        help="Tracking IoU threshold")
    # Congestion / counting
    parser.add_argument("--max-people", type=int, default=100,
                        help="People count that corresponds to 100 % congestion")
    parser.add_argument("--line-start", nargs=2, type=float,
                        default=None,
                        help="Counting line start (norm. x y, 0~1)")
    parser.add_argument("--line-end", nargs=2, type=float,
                        default=None,
                        help="Counting line end   (norm. x y, 0~1)")
    # Snapshot
    parser.add_argument("--cross-vp", type=bool, default=True,
                        help="Enable cross visual prompt mode")
    parser.add_argument("--save-interval", type=int, default=1,
                        help="Interval for saving intermediate frames")
    # Batch processing
    parser.add_argument("--batch-size", type=int, default=4,
                        help="Batch size for inference processing")
    # Image preprocessing
    parser.add_argument("--sharpen", type=float, default=0.0,
                        help="Image sharpening factor (0.0-1.0)")
    parser.add_argument("--contrast", type=float, default=1.1,
                        help="Image contrast factor (0.5-2.0)")
    parser.add_argument("--brightness", type=float, default=0.0,
                        help="Image brightness adjustment (-50 to 50)")
    parser.add_argument("--denoise", type=float, default=0.1,
                        help="Image denoising strength (0.0-1.0)")
    return parser.parse_args()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Geometry helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def point_side(p, a, b) -> int:
    """
    Returns sign of point p relative to oriented line a->b.
    +1 : left side, -1 : right side, 0 : on line
    Using 2-D cross-product.
    """
    x, y = p
    x1, y1 = a
    x2, y2 = b
    val = (x2 - x1) * (y - y1) - (y2 - y1) * (x - x1)
    return 0 if val == 0 else (1 if val > 0 else -1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ObjectMeta Conversion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def convert_results_to_objects(result, class_names) -> list[ObjectMeta]:
    """
    YOLO ê²°ê³¼ë¥¼ ObjectMeta ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Supervision ë°©ì‹ì˜ ë§ˆìŠ¤í¬ ì²˜ë¦¬)
    """
    objects = []
    
    if len(result.boxes) == 0:
        return objects
    
    boxes = result.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]
    confidences = result.boxes.conf.cpu().numpy()
    class_ids = result.boxes.cls.cpu().numpy().astype(int)
    
    # ë§ˆìŠ¤í¬ ì •ë³´ (ìˆì„ ê²½ìš°) - ì •êµí•œ ë³€í™˜ ë¡œì§ ì‚¬ìš©
    masks = None
    if hasattr(result, 'masks') and result.masks is not None:
        try:
            # Supervision ë°©ì‹: masks.xy ì‚¬ìš© (ì´ë¯¸ ì›ë³¸ ì¢Œí‘œê³„ë¡œ ë³€í™˜ë¨)
            if hasattr(result.masks, 'xy') and result.masks.xy is not None:
                # masks.xyëŠ” ì´ë¯¸ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜ëœ polygon ì¢Œí‘œë“¤
                orig_height, orig_width = result.orig_shape
                masks_list = []
                
                for mask_coords in result.masks.xy:
                    # polygonì„ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜
                    mask = np.zeros((orig_height, orig_width), dtype=np.uint8)
                    if len(mask_coords) > 0:
                        # polygon ì¢Œí‘œë¥¼ integerë¡œ ë³€í™˜
                        coords = mask_coords.astype(np.int32)
                        # fillPolyë¡œ ë§ˆìŠ¤í¬ ìƒì„±
                        cv2.fillPoly(mask, [coords], 1)
                    masks_list.append(mask)
                
                masks = np.array(masks_list)
                
            # xyê°€ ì—†ìœ¼ë©´ dataë¥¼ ì‚¬ìš©í•˜ë˜ ë” ì •êµí•œ ë³€í™˜ ì ìš©
            elif hasattr(result.masks, 'data'):
                orig_height, orig_width = result.orig_shape
                mask_data = result.masks.data.cpu().numpy()  # [N, H, W]
                
                # YOLOì˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ë³´ ê³„ì‚°
                input_height, input_width = mask_data.shape[1], mask_data.shape[2]
                
                # aspect ratio ê³„ì‚°
                scale = min(input_width / orig_width, input_height / orig_height)
                scaled_width = int(orig_width * scale)
                scaled_height = int(orig_height * scale)
                
                # íŒ¨ë”© ê³„ì‚°
                pad_x = (input_width - scaled_width) // 2
                pad_y = (input_height - scaled_height) // 2
                
                masks_list = []
                for mask in mask_data:
                    # íŒ¨ë”© ì œê±°
                    if pad_y > 0 and pad_x > 0:
                        mask = mask[pad_y:pad_y + scaled_height, pad_x:pad_x + scaled_width]
                    
                    # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                    resized_mask = cv2.resize(
                        mask.astype(np.float32), 
                        (orig_width, orig_height), 
                        interpolation=cv2.INTER_LINEAR  # ë” ë¶€ë“œëŸ¬ìš´ ë³´ê°„
                    )
                    masks_list.append(resized_mask)
                
                masks = np.array(masks_list)
                
        except Exception as e:
            print(f"âš ï¸ ë§ˆìŠ¤í¬ ë³€í™˜ ì˜¤ë¥˜: {e}")
            masks = None
    
    for i in range(len(boxes)):
        obj = ObjectMeta(
            box=boxes[i],
            mask=masks[i] if masks is not None else None,
            confidence=confidences[i],
            class_id=class_ids[i],
            class_name=class_names[class_ids[i]],
            track_id=None  # íŠ¸ë˜ì»¤ì—ì„œ ë¶€ì—¬ë°›ì„ ì˜ˆì •
        )
        objects.append(obj)
    
    return objects


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Direct Overlay Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def draw_box(image, box, track_id, class_id, color_palette):
    """ë°•ìŠ¤ ê·¸ë¦¬ê¸°"""
    x1, y1, x2, y2 = map(int, box)
    color = color_palette.by_idx(track_id).as_bgr()
    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
    return image

def draw_mask(image, mask, track_id, class_id, color_palette, opacity=0.4):
    """Supervision ë°©ì‹ì˜ ë§ˆìŠ¤í¬ ê·¸ë¦¬ê¸° (ì •êµí•œ í¬ê¸° ë³€í™˜ ì ìš©)"""
    if mask is None:
        return image
    
    color = color_palette.by_idx(track_id).as_bgr()
    
    # ë§ˆìŠ¤í¬ í¬ê¸° ê²€ì¦ ë° ìƒì„¸ ë””ë²„ê¹…
    if mask.shape[:2] != image.shape[:2]:
        print(f"ğŸ” ë§ˆìŠ¤í¬ í¬ê¸° ë””ë²„ê¹…:")
        print(f"   Image: {image.shape[:2]} (H, W)")
        print(f"   Mask:  {mask.shape[:2]} (H, W)")
        print(f"   Track: {track_id}, Class: {class_id}")
        
        # ì •í™•í•œ ë¦¬ì‚¬ì´ì¦ˆ
        mask = cv2.resize(
            mask.astype(np.float32), 
            (image.shape[1], image.shape[0]),  # (width, height) ìˆœì„œ ì£¼ì˜
            interpolation=cv2.INTER_LINEAR
        )
        print(f"   Resized: {mask.shape[:2]}")
    
    # boolean ë§ˆìŠ¤í¬ë¡œ ë³€í™˜ (threshold ì¡°ì •)
    mask = mask > 0.3  # thresholdë¥¼ ë‚®ì¶°ì„œ ë” ë§ì€ ì˜ì—­ í¬í•¨
    
    # ë§ˆìŠ¤í¬ ì˜ì—­ì´ ìˆëŠ”ì§€ í™•ì¸
    mask_area = np.sum(mask)
    if mask_area == 0:
        print(f"âš ï¸ ë¹ˆ ë§ˆìŠ¤í¬: track_id={track_id}")
        return image
    
    # Supervision ë°©ì‹: colored_mask ìƒì„± í›„ addWeighted ì‚¬ìš©
    colored_mask = np.array(image, copy=True, dtype=np.uint8)
    colored_mask[mask] = color  # ë§ˆìŠ¤í¬ ì˜ì—­ì—ë§Œ ìƒ‰ìƒ ì ìš©
    
    # addWeightedë¡œ ë¸”ë Œë”© (ì›ë³¸ ì´ë¯¸ì§€ì— ì§ì ‘ ì ìš©)
    cv2.addWeighted(colored_mask, opacity, image, 1 - opacity, 0, dst=image)
    
    return image

def draw_label(image, box, track_id, class_name, confidence, color_palette):
    """ë¼ë²¨ ê·¸ë¦¬ê¸°"""
    x1, y1, x2, y2 = map(int, box)
    color = color_palette.by_idx(track_id).as_bgr()
    
    # ë¼ë²¨ í…ìŠ¤íŠ¸
    label = f"{class_name} {track_id} {confidence:.2f}"
    
    # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.6
    thickness = 2
    (text_w, text_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)
    
    # ë¼ë²¨ ë°°ê²½ ê·¸ë¦¬ê¸°
    cv2.rectangle(image, (x1, y1 - text_h - baseline - 10), 
                  (x1 + text_w, y1), color, -1)
    
    # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
    cv2.putText(image, label, (x1, y1 - baseline - 5), 
                font, font_scale, (255, 255, 255), thickness)
    
    return image

def draw_objects_overlay(image, objects, color_palette):
    """ëª¨ë“  ê°œì²´ì˜ overlay ê·¸ë¦¬ê¸° (Supervision ë°©ì‹: í° ê°ì²´ë¶€í„° ì‘ì€ ê°ì²´ ìˆœ)"""
    # area ê³„ì‚°í•˜ì—¬ í° ê²ƒë¶€í„° ì •ë ¬ (Supervisionê³¼ ë™ì¼)
    valid_objects = [obj for obj in objects if obj.track_id is not None]
    
    if not valid_objects:
        return image
    
    # ë°•ìŠ¤ area ê³„ì‚°
    areas = []
    for obj in valid_objects:
        x1, y1, x2, y2 = obj.box
        area = (x2 - x1) * (y2 - y1)
        areas.append(area)
    
    # area ê¸°ì¤€ìœ¼ë¡œ í° ê²ƒë¶€í„° ì •ë ¬ (flipìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ)
    sorted_indices = np.flip(np.argsort(areas))
    
    # ë§ˆìŠ¤í¬ë¶€í„° ë¨¼ì € ê·¸ë¦¬ê¸° (supervision ë°©ì‹)
    for idx in sorted_indices:
        obj = valid_objects[idx]
        # ë§ˆìŠ¤í¬ ê·¸ë¦¬ê¸°
        image = draw_mask(image, obj.mask, obj.track_id, obj.class_id, color_palette)
    
    # ê·¸ ë‹¤ìŒ ë°•ìŠ¤ì™€ ë¼ë²¨ ê·¸ë¦¬ê¸° (ì›ë˜ ìˆœì„œëŒ€ë¡œ)
    for obj in valid_objects:
        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        image = draw_box(image, obj.box, obj.track_id, obj.class_id, color_palette)
        
        # ë¼ë²¨ ê·¸ë¦¬ê¸°
        image = draw_label(image, obj.box, obj.track_id, obj.class_name, 
                          obj.confidence, color_palette)
    
    return image


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Batch Processing Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def load_batch_frames(cap, start_frame, end_frame, args):
    """ì§€ì •ëœ ë²”ìœ„ì˜ í”„ë ˆì„ë“¤ì„ batchë¡œ ë¡œë“œ"""
    batch_frames = []
    batch_indices = []
    batch_original_frames = []
    
    for frame_idx in range(start_frame, end_frame):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ok, frame_bgr = cap.read()
        
        if ok:
            # ì›ë³¸ í”„ë ˆì„ ì €ì¥ (ì‹œê°í™”ìš©)
            batch_original_frames.append(frame_bgr.copy())
            
            # RGB ë³€í™˜ ë° ì „ì²˜ë¦¬
            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
            frame_rgb = preprocess_image(frame_rgb, args)  # ê¸°ì¡´ ì „ì²˜ë¦¬ ìœ ì§€
            
            # PIL Imageë¡œ ë³€í™˜ (YOLO ì…ë ¥ í˜•ì‹)
            batch_frames.append(Image.fromarray(frame_rgb))
            batch_indices.append(frame_idx)
    
    return batch_frames, batch_indices, batch_original_frames


def inference_batch(batch_frames, model, model_vp, prev_vpe, args):
    """Batch ë‹¨ìœ„ë¡œ inference ìˆ˜í–‰"""
    
    if prev_vpe is not None and args.cross_vp:
        # ì´ì „ batchì˜ VPEë¥¼ í˜„ì¬ batchì— ì ìš©
        model_vp.set_classes(args.names, prev_vpe)
        model_vp.predictor = None
        
        # Batch inference with VPE
        results = model_vp.predict(
            source=batch_frames,  # ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ë©´ batch ì²˜ë¦¬ë¨
            imgsz=args.image_size,
            conf=args.conf_thresh,
            iou=args.iou_thresh,
            verbose=False
        )
    else:
        # ì²« ë²ˆì§¸ batch ë˜ëŠ” VPE ì—†ì´ ì²˜ë¦¬
        results = model.predict(
            source=batch_frames,
            imgsz=args.image_size,
            conf=0.05,
            iou=args.iou_thresh,
            verbose=False
        )
    
    return results


def update_batch_vpe(batch_results, model_vp, args):
    """Batch ê²°ê³¼ì—ì„œ VPE ìƒì„± ë° í‰ê· í™”"""
    
    high_conf_prompts = []
    
    print(f"ğŸ” VPE ìƒì„±ì„ ìœ„í•œ high-confidence detection ìˆ˜ì§‘ ì¤‘...")
    
    # Batch ë‚´ ëª¨ë“  í”„ë ˆì„ì—ì„œ high-confidence detection ìˆ˜ì§‘
    for i, result in enumerate(batch_results):
        if len(result.boxes) > 0:
            confidences = result.boxes.conf.cpu().numpy()
            boxes = result.boxes.xyxy.cpu().numpy()
            class_ids = result.boxes.cls.cpu().numpy()
            
            high_conf_mask = confidences >= args.vp_thresh
            
            if np.any(high_conf_mask):
                # High confidence detectionì„ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
                prompt_data = {
                    "bboxes": boxes[high_conf_mask],
                    "cls": class_ids[high_conf_mask],
                    "frame": result.orig_img,  # ì›ë³¸ ì´ë¯¸ì§€ ì •ë³´
                    "frame_idx": i
                }
                
                # ëª¨ë“  í´ë˜ìŠ¤ê°€ í¬í•¨ë˜ë„ë¡ ì¶”ê°€
                for cls_idx in range(len(args.names)):
                    if cls_idx not in class_ids:
                        prompt_data["bboxes"] = np.append(prompt_data["bboxes"], [[0, 0, 0, 0]], axis=0)
                        prompt_data["cls"] = np.append(prompt_data["cls"], [cls_idx])                
                        
                high_conf_prompts.append(prompt_data)
                print(f"   í”„ë ˆì„ {i}: {len(boxes[high_conf_mask])} ê°œì˜ high-conf detection ìˆ˜ì§‘")
                
    if high_conf_prompts:
        print(f"âœ… ì´ {len(high_conf_prompts)} í”„ë ˆì„ì—ì„œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
        # Batch ë‚´ í”„ë¡¬í”„íŠ¸ë“¤ë¡œ VPE ìƒì„±
        batch_vpe = generate_batch_vpe(high_conf_prompts, model_vp, args)
        return batch_vpe
    else:
        print("âš ï¸ High-confidence detectionì´ ì—†ì–´ VPE ìƒì„± ë¶ˆê°€")
        return None


def generate_batch_vpe(prompts_list, model_vp, args):
    """ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ì—ì„œ VPEë¥¼ ë°°ì¹˜ë¡œ ìƒì„± í›„ í‰ê· í™”"""
    
    print(f"ğŸ§  {len(prompts_list)} ê°œ í”„ë¡¬í”„íŠ¸ì—ì„œ ë°°ì¹˜ VPE ìƒì„± ì¤‘...")
    
    # ë°°ì¹˜ìš© ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    batch_images = []
    batch_prompts = {"bboxes": [], "cls": []}
    
    for i, prompt_data in enumerate(prompts_list):
        # ì´ë¯¸ì§€ ì¤€ë¹„
        frame_rgb = cv2.cvtColor(prompt_data["frame"], cv2.COLOR_BGR2RGB)
        batch_images.append(Image.fromarray(frame_rgb))
        
        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        batch_prompts["bboxes"].append(prompt_data["bboxes"])
        batch_prompts["cls"].append(prompt_data["cls"])
    
    try:
        # ë°°ì¹˜ VPE ìƒì„± (í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬)
        print(f"   ğŸ“¦ {len(batch_images)} ê°œ ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ VPE ìƒì„± ì¤‘...")
        
        model_vp.predictor = None  # VPE ìƒì„± ì „ ì´ˆê¸°í™”
        model_vp.predict(
            source=batch_images,  # â† ë°°ì¹˜ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            prompts=batch_prompts,  # â† ë°°ì¹˜ í”„ë¡¬í”„íŠ¸
            predictor=YOLOEVPSegPredictor,
            return_vpe=True,
            imgsz=args.image_size,
            conf=args.vp_thresh,
            iou=args.iou_thresh,
            verbose=False
        )
        
        # VPE ì¶”ì¶œ
        if hasattr(model_vp, 'predictor') and hasattr(model_vp.predictor, 'vpe'):
            batch_vpe = model_vp.predictor.vpe
            print(f"     âœ… ë°°ì¹˜ VPE ìƒì„± ì„±ê³µ (shape: {batch_vpe.shape})")
            
            # ë°°ì¹˜ ì°¨ì›ì„ í‰ê· í™”í•˜ì—¬ ë‹¨ì¼ VPEë¡œ ë³€í™˜
            if len(batch_vpe.shape) > 2:  # (batch_size, classes, features) í˜•íƒœì¸ ê²½ìš°
                averaged_vpe = batch_vpe.mean(dim=0, keepdim=True)  # (1, classes, features)
                print(f"     ğŸ”„ ë°°ì¹˜ VPE í‰ê· í™”: {batch_vpe.shape} â†’ {averaged_vpe.shape}")
            else:
                averaged_vpe = batch_vpe
            
            # ì˜ˆì¸¡ê¸° ì •ë¦¬
            model_vp.predictor = None
            
            return averaged_vpe
        else:
            print(f"     âŒ ë°°ì¹˜ VPE ìƒì„± ì‹¤íŒ¨ - predictor ë˜ëŠ” vpe ì†ì„± ì—†ìŒ")
            model_vp.predictor = None
            return None
            
    except Exception as e:
        print(f"     âŒ ë°°ì¹˜ VPE ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        model_vp.predictor = None
        return None


def process_batch_results(batch_results, batch_indices, batch_original_frames, 
                         tracker, args, fps, palette, person_class_id,
                         # ìƒíƒœ ë³€ìˆ˜ë“¤
                         track_history, track_side, track_color, 
                         forward_cnt, backward_cnt, current_occupancy, current_congestion,
                         last_update_time, heatmap, last_save_frame, log_buffer,
                         # I/O ê´€ë ¨
                         output_dir, out, log_file,
                         # ë¼ì¸ í¬ë¡œì‹± ê´€ë ¨
                         p1, p2, seg_dx, seg_dy, seg_len2,
                         width, height):
    """Batch ê²°ê³¼ë¥¼ ê°œë³„ í”„ë ˆì„ìœ¼ë¡œ ì²˜ë¦¬"""
    
    updated_state = {
        'forward_cnt': forward_cnt,
        'backward_cnt': backward_cnt,
        'current_occupancy': current_occupancy,
        'current_congestion': current_congestion,
        'last_update_time': last_update_time,
        'last_save_frame': last_save_frame
    }
    
    for result, frame_idx, original_frame in zip(batch_results, batch_indices, batch_original_frames):
        # ì‹œê°„ ê³„ì‚°
        current_time = frame_idx / fps
        hours = int(current_time // 3600)
        minutes = int((current_time % 3600) // 60)
        seconds = int(current_time % 60)
        milliseconds = int((current_time % 1) * 1000)
        time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"
        
        # ObjectMeta ë³€í™˜
        detected_objects = convert_results_to_objects(result, args.names)
        
        # Tracker ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        tracked_objects = tracker.update(detected_objects, result.orig_img, "None")
        
        # ê¸°ì¡´ ë³€ìˆ˜ë“¤ ì¶”ì¶œ (í˜¸í™˜ì„± ìœ ì§€)
        if tracked_objects:
            boxes_xywh = []
            for obj in tracked_objects:
                x1, y1, x2, y2 = obj.box  # xyxy
                cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
                w, h = x2 - x1, y2 - y1
                boxes_xywh.append([cx, cy, w, h])
            boxes_xywh = np.array(boxes_xywh)
            
            class_ids = np.array([obj.class_id for obj in tracked_objects])
            track_ids = np.array([obj.track_id for obj in tracked_objects])
        else:
            boxes_xywh = np.empty((0, 4))
            class_ids = np.empty(0, int)
            track_ids = np.empty(0, int)

        # ë¡œê¹… ì²˜ë¦¬
        if log_file is not None:
            class_counts = defaultdict(int)
            for cid in class_ids:
                class_counts[cid] += 1

            print(f"Frame {frame_idx} - Class counts:")
            for cid, count in class_counts.items():
                print(f"  {args.names[cid]}: {count}")

            for cid, tid in zip(class_ids, track_ids):
                class_name = args.names[cid]
                class_count = class_counts[cid]
                log_entry = f"{time_str},{class_name},{tid},{class_count}\n"
                log_buffer.append(log_entry)
                if len(log_buffer) >= 100:
                    log_file.writelines(log_buffer)
                    log_file.flush()
                    log_buffer.clear()

        # ì‹œê°í™”
        frame_rgb = cv2.cvtColor(original_frame, cv2.COLOR_BGR2RGB)
        annotated = frame_rgb.copy()
        if tracked_objects:
            annotated = draw_objects_overlay(annotated, tracked_objects, palette)

        # Occupancy ì—…ë°ì´íŠ¸
        raw_occupancy = int(np.sum(class_ids == person_class_id))
        if current_time - updated_state['last_update_time'] >= 1.0:
            updated_state['last_update_time'] = current_time
            updated_state['current_occupancy'] = raw_occupancy
            updated_state['current_congestion'] = int(min(raw_occupancy / max(args.max_people, 1), 1.0) * 100)

        line_crossed_this_frame = False

        # íˆíŠ¸ë§µ ë° ë¼ì¸ í¬ë¡œì‹± ì²˜ë¦¬
        for obj in tracked_objects:
            if obj.track_id == -1 or obj.class_id != person_class_id:
                continue

            x1, y1, x2, y2 = obj.box
            cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
            ix, iy = int(cx), int(cy)
            
            # íˆíŠ¸ë§µ ì—…ë°ì´íŠ¸
            if 0 <= iy < height and 0 <= ix < width:
                radius = 10
                for dy in range(-radius, radius + 1):
                    for dx in range(-radius, radius + 1):
                        if dx*dx + dy*dy <= radius*radius:
                            ny, nx = iy + dy, ix + dx
                            if 0 <= ny < height and 0 <= nx < width:
                                intensity = 1.0 * (1.0 - ((dx*dx + dy*dy) / (radius*radius)))
                                heatmap[ny, nx] += intensity

            # íŠ¸ë˜í‚¹ ì»¬ëŸ¬ ì„¤ì •
            if obj.track_id not in track_color:
                track_color[obj.track_id] = palette.by_idx(obj.track_id).as_bgr()

            # ë¼ì¸ í¬ë¡œì‹± ì²´í¬
            if p1 is not None and p2 is not None:
                side_now = point_side((cx, cy), p1, p2)
                t = ((cx - p1[0]) * seg_dx + (cy - p1[1]) * seg_dy) / seg_len2
                inside = 0.0 <= t <= 1.0

                side_prev = track_side.get(obj.track_id, side_now)
                if inside and (side_prev * side_now < 0):
                    line_crossed_this_frame = True
                    if side_prev < 0 < side_now:
                        updated_state['forward_cnt'] += 1
                    elif side_prev > 0 > side_now:
                        updated_state['backward_cnt'] += 1
                if inside and side_now != 0:
                    track_side[obj.track_id] = side_now

        # ë¼ì¸ ê·¸ë¦¬ê¸°
        if p1 is not None and p2 is not None:
            if line_crossed_this_frame:
                cv2.line(annotated, p1, p2, (255, 0, 0), 8)
            else:
                cv2.line(annotated, p1, p2, (255, 255, 255), 8)

            mid_x, mid_y = (p1[0] + p2[0]) // 2, (p1[1] + p2[1]) // 2
            dx, dy = p2[0] - p1[0], p2[1] - p1[1]
            perp_x, perp_y = -dy, dx
            length = 50
            mag = (perp_x**2 + perp_y**2) ** 0.5 or 1
            perp_x, perp_y = int(perp_x / mag * length), int(perp_y / mag * length)
            arrow_start, arrow_end = (mid_x, mid_y), (mid_x + perp_x, mid_y + perp_y)
            cv2.arrowedLine(annotated, arrow_start, arrow_end, (0, 255, 0), 8, tipLength=0.6)

        # Overlay ì •ë³´ ê·¸ë¦¬ê¸°
        overlay = [
            f"[1] Congestion : {updated_state['current_congestion']:3d} %",
            f"[2] Crossing   : forward {updated_state['forward_cnt']} | backward {updated_state['backward_cnt']}",
            f"[3] Occupancy  : {updated_state['current_occupancy']}",
            f"[4] Time      : {time_str}"
        ]
        x0, y0, dy = 30, 60, 50
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1
        font_thickness = 2
        padding = 10
        max_width = 0
        for txt in overlay:
            (tw, th), _ = cv2.getTextSize(txt, font, font_scale, font_thickness)
            max_width = max(max_width, tw)
        rect_x1, rect_y1 = x0 - padding, y0 - th - padding
        rect_x2, rect_y2 = x0 + max_width + padding, y0 + (len(overlay) - 1) * dy + padding
        overlay_rect = annotated.copy()
        cv2.rectangle(overlay_rect, (rect_x1, rect_y1), (rect_x2, rect_y2), (0, 0, 0), -1)
        annotated = cv2.addWeighted(overlay_rect, 0.4, annotated, 0.6, 0)
        for i, txt in enumerate(overlay):
            cv2.putText(annotated, txt, (x0, y0 + i * dy),
                        font, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)

        # íˆíŠ¸ë§µ minimap
        blur = cv2.GaussianBlur(heatmap, (0, 0), sigmaX=15, sigmaY=15)
        norm = cv2.normalize(blur, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        color_hm = cv2.applyColorMap(norm, cv2.COLORMAP_JET)
        color_hm = cv2.cvtColor(color_hm, cv2.COLOR_BGR2RGB)

        mini_w = 400
        mini_h = int(height * mini_w / width)
        mini_map = cv2.resize(color_hm, (mini_w, mini_h))

        margin = 20
        x_start = width - mini_w - margin
        y_start = margin
        roi = annotated[y_start:y_start + mini_h, x_start:x_start + mini_w]
        blended = cv2.addWeighted(mini_map, 0.6, roi, 0.4, 0)
        annotated[y_start:y_start + mini_h, x_start:x_start + mini_w] = blended

        # ì¤‘ê°„ ì €ì¥
        if frame_idx - updated_state['last_save_frame'] >= args.save_interval:
            snapshot_path = os.path.join(output_dir, f"intermediate_{frame_idx:06d}_{time_str.replace(':', '-')}.jpg")
            cv2.imwrite(snapshot_path, cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))
            updated_state['last_save_frame'] = frame_idx
            print(f"\nì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {snapshot_path}")

        # ë¹„ë””ì˜¤ ì¶œë ¥
        out.write(cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))
    
    return updated_state


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def preprocess_image(image, args):
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    # ì´ë¯¸ì§€ë¥¼ float32ë¡œ ë³€í™˜
    img = image.astype(np.float32)
    
    # ì„ ëª…ë„ í–¥ìƒ
    if args.sharpen > 0:
        kernel = np.array([[-1,-1,-1],
                          [-1, 9,-1],
                          [-1,-1,-1]]) * args.sharpen
        img = cv2.filter2D(img, -1, kernel)
    
    # ëŒ€ë¹„ ì¡°ì •
    if args.contrast != 1.0:
        img = cv2.convertScaleAbs(img, alpha=args.contrast, beta=0)
    
    # ë°ê¸° ì¡°ì •
    if args.brightness != 0:
        img = cv2.convertScaleAbs(img, alpha=1.0, beta=args.brightness)
    
    # ë…¸ì´ì¦ˆ ì œê±°
    if args.denoise > 0:
        img = cv2.fastNlMeansDenoisingColored(img, None, 
                                             h=args.denoise*10, 
                                             hColor=args.denoise*10, 
                                             templateWindowSize=7, 
                                             searchWindowSize=21)
    
    # ê°’ ë²”ìœ„ë¥¼ 0-255ë¡œ ì œí•œ
    img = np.clip(img, 0, 255).astype(np.uint8)
    return img

def main() -> None:
    args = parse_args()
    # Hybrid VP íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’ 10, 10)
    KEY_INT = getattr(args, 'key_int', 10)
    HOLD_FR = getattr(args, 'hold', 10)
    drift_max = HOLD_FR  # ì—°ì† ê²€ì¶œ ì‹¤íŒ¨ì‹œ ë¦¬ì…‹
    drift_count = 0

    # ì…ë ¥ íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ìë¥¼ ì œì™¸í•œ ê¸°ë³¸ ì´ë¦„ ì¶”ì¶œ
    base_name = os.path.splitext(os.path.basename(args.source))[0]
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output is None:
        output_dir = os.path.join(os.path.dirname(args.source), base_name)
    else:
        output_dir = args.output
    
    # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ìˆë‹¤ë©´ ì‚­ì œ
    if os.path.exists(output_dir):
        import shutil
        print(f"ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘: {output_dir}")
        shutil.rmtree(output_dir)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    print(f"ìƒˆë¡œìš´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    
    # ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ ì„¤ì •
    output_video = os.path.join(output_dir, f"{base_name}_output.mp4")

    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    log_file = None
    if args.log_detections:
        log_path = os.path.join(output_dir, "label.txt")
        log_file = open(log_path, "w", encoding="utf-8")
        log_file.write("Frame_Time,Class,TrackID,Class_Count\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ I/O setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    cap = cv2.VideoCapture(args.source)
    if not cap.isOpened():
        raise FileNotFoundError(f"Cannot open video {args.source}")

    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps, total_frames = cap.get(cv2.CAP_PROP_FPS), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Select frames to process
    frames_to_process = range(total_frames)
    
    # Batch ì²˜ë¦¬ ì„¤ì •
    batch_size = args.batch_size
    total_batches = (total_frames + batch_size - 1) // batch_size
    print(f"ğŸš€ Batch ì²˜ë¦¬ ì‹œì‘: {total_frames} í”„ë ˆì„ì„ {batch_size} ë‹¨ìœ„ë¡œ {total_batches} batch ì²˜ë¦¬")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Model & palette â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    model = YOLOE(args.checkpoint)
    model.eval()
    model.to(args.device)
    model.set_classes(args.names, model.get_text_pe(args.names))

    # VP ëª¨ë¸ ë³„ë„ ë¡œë“œ
    model_vp = YOLOE(args.checkpoint)
    model_vp.eval()
    model_vp.to(args.device)
    model_vp.set_classes(args.names, model_vp.get_text_pe(args.names))

    person_class_id = args.names.index("fish") if "fish" in args.names else 0
    palette = sv.ColorPalette.DEFAULT
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    tracker = BoostTrack(
        video_name=args.source,
        det_thresh=args.track_det_thresh,
        iou_threshold=args.track_iou_thresh
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Counting line (pixel) â”€â”€â”€â”€â”€ #
    if args.line_start is not None and args.line_end is not None:
        p1 = (int(args.line_end[0]   * width),  int(args.line_end[1]   * height))
        p2 = (int(args.line_start[0] * width),  int(args.line_start[1] * height))
        
        # ì„ ë¶„ ë²¡í„° ë° ê¸¸ì´Â² ê³„ì‚°
        seg_dx, seg_dy = p2[0] - p1[0], p2[1] - p1[1]
        seg_len2 = seg_dx * seg_dx + seg_dy * seg_dy or 1
    else:
        p1 = p2 = None
        seg_dx = seg_dy = seg_len2 = 0

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Runtime state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    track_history = defaultdict(list)
    track_side   = {}
    track_color  = {}
    forward_cnt = backward_cnt = 0

    # íŠ¸ë˜í‚¹ ë¼ì¸ ê´€ë ¨ ë³€ìˆ˜
    max_track_history = 30  # ìµœëŒ€ íŠ¸ë˜í‚¹ íˆìŠ¤í† ë¦¬ ê¸¸ì´
    line_thickness = 2     # ë¼ì¸ ë‘ê»˜
    line_opacity = 0.5     # ë¼ì¸ íˆ¬ëª…ë„

    # --- í”„ë¡¬í”„íŠ¸ ë°˜ë³µ ë¡œì§ìš© ìƒíƒœ ë³€ìˆ˜ --- #
    prev_prompt = None
    prompt_frame = None
    last_vp_frame = None
    last_vp_results = None
    
    # VPE moving average ìƒíƒœ ë³€ìˆ˜
    vpe_avg = None
    vpe_alpha = 0.9  # moving average ê³„ìˆ˜ (0.9 = ì´ì „ ê°’ì˜ 90% + í˜„ì¬ ê°’ì˜ 10%)

    # Heat-map ëˆ„ì  ë²„í¼
    heatmap = np.zeros((height, width), dtype=np.float32)

    # ë¼ì¸ ê¹œë°•ì„ ìƒíƒœ
    line_flash = False

    # 1ì´ˆë§ˆë‹¤ ê°±ì‹ ë˜ëŠ” ê°’ë“¤
    last_update_time = 0
    current_occupancy = 0
    current_congestion = 0
    frame_count = 0

    # ì¤‘ê°„ ì €ì¥ ê´€ë ¨ ë³€ìˆ˜
    last_save_frame = 0
    log_buffer = []  # ë¡œê·¸ ë²„í¼

    # ë¹„ë””ì˜¤ ì¶œë ¥ ì„¤ì •
    out = cv2.VideoWriter(output_video,
                        cv2.VideoWriter_fourcc(*'mp4v'),
                        fps, (width, height))

    pbar = tqdm(total=total_batches, desc="Processing batches")
    
    # VPE ìƒíƒœ ë³€ìˆ˜
    prev_vpe = None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Batch loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    for batch_idx in range(total_batches):
        batch_start = batch_idx * batch_size
        batch_end = min(batch_start + batch_size, total_frames)
        
        print(f"\nğŸ“¦ Batch {batch_idx + 1}/{total_batches}: í”„ë ˆì„ {batch_start}-{batch_end-1}")
        
        # 1. Batch í”„ë ˆì„ ë¡œë“œ
        batch_frames, batch_indices, batch_original_frames = load_batch_frames(
            cap, batch_start, batch_end, args)
        
        if not batch_frames:
            print("âš ï¸ ë¹ˆ batch, ê±´ë„ˆë›°ê¸°")
            continue
            
        # 2. Batch inference
        batch_results = inference_batch(batch_frames, model, model_vp, prev_vpe, args)
        
        # 2.5. VPE ì—…ë°ì´íŠ¸ (ë‹¤ìŒ batchìš©) - cross_vp í™œì„±í™”ì‹œë§Œ
        if args.cross_vp:
            print(f"ğŸ”„ Batch {batch_idx + 1}ì—ì„œ ë‹¤ìŒ batchìš© VPE ìƒì„± ì¤‘...")
            current_vpe = update_batch_vpe(batch_results, model_vp, args)
            
            if current_vpe is not None:
                if prev_vpe is None:
                    # ì²« ë²ˆì§¸ VPE
                    prev_vpe = current_vpe
                    print(f"ğŸ¯ ì²« ë²ˆì§¸ VPE ì„¤ì • ì™„ë£Œ")
                else:
                    # VPE Moving Average (momentum=0.7)
                    momentum = 0.7
                    prev_vpe = momentum * prev_vpe + (1 - momentum) * current_vpe
                    print(f"ğŸ”„ VPE Moving Average ì—…ë°ì´íŠ¸ ì™„ë£Œ (momentum={momentum})")
            else:
                print(f"âš ï¸ Batch {batch_idx + 1}ì—ì„œ VPE ìƒì„± ì‹¤íŒ¨, ì´ì „ VPE ìœ ì§€")
        else:
            print(f"ğŸš« Cross-VP ë¹„í™œì„±í™”ë¨, VPE ìƒì„± ê±´ë„ˆë›°ê¸°")
        
        # 3. Batch ê²°ê³¼ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        updated_state = process_batch_results(
            batch_results, batch_indices, batch_original_frames,
            tracker, args, fps, palette, person_class_id,
            # ìƒíƒœ ë³€ìˆ˜ë“¤
            track_history, track_side, track_color, 
            forward_cnt, backward_cnt, current_occupancy, current_congestion,
            last_update_time, heatmap, last_save_frame, log_buffer,
            # I/O ê´€ë ¨
            output_dir, out, log_file,
            # ë¼ì¸ í¬ë¡œì‹± ê´€ë ¨
            p1, p2, seg_dx, seg_dy, seg_len2,
            width, height
        )
        
        # ìƒíƒœ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        forward_cnt = updated_state['forward_cnt']
        backward_cnt = updated_state['backward_cnt']
        current_occupancy = updated_state['current_occupancy'] 
        current_congestion = updated_state['current_congestion']
        last_update_time = updated_state['last_update_time']
        last_save_frame = updated_state['last_save_frame']
        
        # Progress bar ì—…ë°ì´íŠ¸
        pbar.update(1)
        vpe_status = "ON" if prev_vpe is not None else "OFF"
        pbar.set_postfix({
            'VPE': vpe_status,
            'occupancy': current_occupancy,
            'congestion': f"{current_congestion}%",
            'forward': forward_cnt,
            'backward': backward_cnt
        })
        
        print(f"âœ… Batch {batch_idx + 1} ì™„ë£Œ: {len(batch_results)} í”„ë ˆì„ ì²˜ë¦¬")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Batch ì²˜ë¦¬ ì™„ë£Œ & ì •ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    print(f"\nğŸ‰ ëª¨ë“  batch ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… í†µê³„:")
    print(f"   - ì´ ì²˜ë¦¬ëœ batch: {total_batches}")
    print(f"   - ì´ ì²˜ë¦¬ëœ í”„ë ˆì„: {total_frames}")
    print(f"   - VPE ìƒíƒœ: {'í™œì„±í™”' if prev_vpe is not None else 'ë¹„í™œì„±í™”'}")
    print(f"   - Batch í¬ê¸°: {batch_size}")
    print(f"   - ìµœì¢… occupancy: {current_occupancy}")
    print(f"   - ìµœì¢… congestion: {current_congestion}%")
    print(f"   - ë¼ì¸ í¬ë¡œì‹±: forward {forward_cnt}, backward {backward_cnt}")
    
    if args.cross_vp and prev_vpe is not None:
        print(f"ğŸ§  VPE ì •ë³´:")
        print(f"   - VPE shape: {prev_vpe.shape}")
        print(f"   - VPE dtype: {prev_vpe.dtype}")
        print(f"   - Cross-VP ëª¨ë“œ: í™œì„±í™”")

    pbar.close()
    cap.release()
    if log_file is not None:
        if log_buffer:
            log_file.writelines(log_buffer)
        log_file.close()
    out.release()
    print(f"âœ” ì™„ë£Œ. ì €ì¥ ìœ„ì¹˜: {output_dir}")


if __name__ == "__main__":
    main() 
