#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Video people-tracking + congestion, line-cross counting, occupancy overlay
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
* Congestion   : 0-100 % = (min(occupancy / max_people, 1) Ã— 100)
* People count : forward / backward crossings over a user-defined line
* Occupancy    : current # of tracked persons in frame
* Heat-map     : cumulative person locations, shown as minimap (top-right)
* Colours      : Supervision TRACK palette (same as masks / labels)
* Requirements : supervision â‰¥ 0.21, ultralytics (YOLOE fork) + BoT-SORT
"""
import argparse
import os
import random
from collections import defaultdict

import cv2
import numpy as np
from PIL import Image
from tqdm import tqdm
import supervision as sv
from ultralytics import YOLOE
from ultralytics.models.yolo.yoloe.predict_vp import YOLOEVPSegPredictor
import torch

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
from tracker.boostTrack.GBI import GBInterpolation
from tracker.boostTrack.boost_track import BoostTrack


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ObjectMeta Class â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
class ObjectMeta:
    """
    ê°œì²´ì˜ ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ ë‹´ëŠ” í´ë˜ìŠ¤
    Detectionë¶€í„° Tracking, Overlayê¹Œì§€ ì¼ê´€ë˜ê²Œ ì‚¬ìš©
    """
    def __init__(self, box=None, mask=None, confidence=None, class_id=None, 
                 class_name=None, track_id=None):
        self.box = box              # [x1, y1, x2, y2] or [cx, cy, w, h]
        self.mask = mask            # segmentation mask array
        self.confidence = confidence # detection confidence score
        self.class_id = class_id    # class index
        self.class_name = class_name # class name string
        self.track_id = track_id    # tracking ID (None initially)
    
    def __repr__(self):
        return f"ObjectMeta(class={self.class_name}, track_id={self.track_id}, conf={self.confidence:.2f})"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    # I/O
    parser.add_argument("--source", type=str, default="/DL_data_super_hdd/video_label_sandbox/Heungchuk_videos/day.mp4",
                        help="Input video path")
    parser.add_argument("--output", type=str, default=None,
                        help="Output directory (optional, defaults to input filename without extension)")
    # Logging
    parser.add_argument("--log-detections", type=bool, default=True,
                        help="Log detection results to label.txt")
    # Model
    parser.add_argument("--checkpoint", type=str,
                        default="/works/samsung_prj/pretrain/yoloe-11l-seg.pt",
                        help="YOLOE checkpoint (detection + seg)")
    parser.add_argument("--names", nargs="+",
                        default=["cow", "cattle"],
                        help="Custom class names list (index order matters)")
    parser.add_argument("--device", type=str, default="cuda:0",
                        help="Inference device")
    # Inference / tracking
    parser.add_argument("--image-size", type=int, default=640,
                        help="Inference image size")
    parser.add_argument("--conf-thresh", type=float, default=0.1,
                        help="Detection confidence threshold")
    parser.add_argument("--vp-thresh", type=float, default=0.1,
                        help="visual prompt confidence threshold")
    parser.add_argument("--iou-thresh", type=float, default=0.45,
                        help="Detection NMS IoU threshold")
    parser.add_argument("--track-history", type=int, default=100,
                        help="Frames kept in trajectory history")
    parser.add_argument("--track-det-thresh", type=float, default=0.1,
                        help="Detection confidence threshold for tracking")
    parser.add_argument("--track-iou-thresh", type=float, default=0.3,
                        help="Tracking IoU threshold")
    # Congestion / counting
    parser.add_argument("--max-people", type=int, default=100,
                        help="People count that corresponds to 100 % congestion")
    parser.add_argument("--line-start", nargs=2, type=float,
                        default=None,
                        help="Counting line start (norm. x y, 0~1)")
    parser.add_argument("--line-end", nargs=2, type=float,
                        default=None,
                        help="Counting line end   (norm. x y, 0~1)")
    # Snapshot
    parser.add_argument("--cross-vp", type=bool, default=True,
                        help="Enable cross visual prompt mode")
    parser.add_argument("--save-interval", type=int, default=1,
                        help="Interval for saving intermediate frames")
    # Image preprocessing
    parser.add_argument("--sharpen", type=float, default=0.0,
                        help="Image sharpening factor (0.0-1.0)")
    parser.add_argument("--contrast", type=float, default=1.1,
                        help="Image contrast factor (0.5-2.0)")
    parser.add_argument("--brightness", type=float, default=0.0,
                        help="Image brightness adjustment (-50 to 50)")
    parser.add_argument("--denoise", type=float, default=0.1,
                        help="Image denoising strength (0.0-1.0)")
    return parser.parse_args()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Geometry helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def point_side(p, a, b) -> int:
    """
    Returns sign of point p relative to oriented line a->b.
    +1 : left side, -1 : right side, 0 : on line
    Using 2-D cross-product.
    """
    x, y = p
    x1, y1 = a
    x2, y2 = b
    val = (x2 - x1) * (y - y1) - (y2 - y1) * (x - x1)
    return 0 if val == 0 else (1 if val > 0 else -1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ObjectMeta Conversion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def convert_results_to_objects(result, class_names) -> list[ObjectMeta]:
    """
    YOLO ê²°ê³¼ë¥¼ ObjectMeta ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Supervision ë°©ì‹ì˜ ë§ˆìŠ¤í¬ ì²˜ë¦¬)
    """
    objects = []
    
    if len(result.boxes) == 0:
        return objects
    
    boxes = result.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]
    confidences = result.boxes.conf.cpu().numpy()
    class_ids = result.boxes.cls.cpu().numpy().astype(int)
    
    # ë§ˆìŠ¤í¬ ì •ë³´ (ìˆì„ ê²½ìš°) - ì •êµí•œ ë³€í™˜ ë¡œì§ ì‚¬ìš©
    masks = None
    if hasattr(result, 'masks') and result.masks is not None:
        try:
            # Supervision ë°©ì‹: masks.xy ì‚¬ìš© (ì´ë¯¸ ì›ë³¸ ì¢Œí‘œê³„ë¡œ ë³€í™˜ë¨)
            if hasattr(result.masks, 'xy') and result.masks.xy is not None:
                # masks.xyëŠ” ì´ë¯¸ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜ëœ polygon ì¢Œí‘œë“¤
                orig_height, orig_width = result.orig_shape
                masks_list = []
                
                for mask_coords in result.masks.xy:
                    # polygonì„ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜
                    mask = np.zeros((orig_height, orig_width), dtype=np.uint8)
                    if len(mask_coords) > 0:
                        # polygon ì¢Œí‘œë¥¼ integerë¡œ ë³€í™˜
                        coords = mask_coords.astype(np.int32)
                        # fillPolyë¡œ ë§ˆìŠ¤í¬ ìƒì„±
                        cv2.fillPoly(mask, [coords], 1)
                    masks_list.append(mask)
                
                masks = np.array(masks_list)
                
            # xyê°€ ì—†ìœ¼ë©´ dataë¥¼ ì‚¬ìš©í•˜ë˜ ë” ì •êµí•œ ë³€í™˜ ì ìš©
            elif hasattr(result.masks, 'data'):
                orig_height, orig_width = result.orig_shape
                mask_data = result.masks.data.cpu().numpy()  # [N, H, W]
                
                # YOLOì˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ë³´ ê³„ì‚°
                input_height, input_width = mask_data.shape[1], mask_data.shape[2]
                
                # aspect ratio ê³„ì‚°
                scale = min(input_width / orig_width, input_height / orig_height)
                scaled_width = int(orig_width * scale)
                scaled_height = int(orig_height * scale)
                
                # íŒ¨ë”© ê³„ì‚°
                pad_x = (input_width - scaled_width) // 2
                pad_y = (input_height - scaled_height) // 2
                
                masks_list = []
                for mask in mask_data:
                    # íŒ¨ë”© ì œê±°
                    if pad_y > 0 and pad_x > 0:
                        mask = mask[pad_y:pad_y + scaled_height, pad_x:pad_x + scaled_width]
                    
                    # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                    resized_mask = cv2.resize(
                        mask.astype(np.float32), 
                        (orig_width, orig_height), 
                        interpolation=cv2.INTER_LINEAR  # ë” ë¶€ë“œëŸ¬ìš´ ë³´ê°„
                    )
                    masks_list.append(resized_mask)
                
                masks = np.array(masks_list)
                
        except Exception as e:
            print(f"âš ï¸ ë§ˆìŠ¤í¬ ë³€í™˜ ì˜¤ë¥˜: {e}")
            masks = None
    
    for i in range(len(boxes)):
        obj = ObjectMeta(
            box=boxes[i],
            mask=masks[i] if masks is not None else None,
            confidence=confidences[i],
            class_id=class_ids[i],
            class_name=class_names[class_ids[i]],
            track_id=None  # íŠ¸ë˜ì»¤ì—ì„œ ë¶€ì—¬ë°›ì„ ì˜ˆì •
        )
        objects.append(obj)
    
    return objects


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Direct Overlay Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def draw_box(image, box, track_id, class_id, color_palette):
    """ë°•ìŠ¤ ê·¸ë¦¬ê¸°"""
    x1, y1, x2, y2 = map(int, box)
    color = color_palette.by_idx(track_id).as_bgr()
    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
    return image

def draw_mask(image, mask, track_id, class_id, color_palette, opacity=0.4):
    """Supervision ë°©ì‹ì˜ ë§ˆìŠ¤í¬ ê·¸ë¦¬ê¸° (ì •êµí•œ í¬ê¸° ë³€í™˜ ì ìš©)"""
    if mask is None:
        return image
    
    color = color_palette.by_idx(track_id).as_bgr()
    
    # ë§ˆìŠ¤í¬ í¬ê¸° ê²€ì¦ ë° ìƒì„¸ ë””ë²„ê¹…
    if mask.shape[:2] != image.shape[:2]:
        print(f"ğŸ” ë§ˆìŠ¤í¬ í¬ê¸° ë””ë²„ê¹…:")
        print(f"   Image: {image.shape[:2]} (H, W)")
        print(f"   Mask:  {mask.shape[:2]} (H, W)")
        print(f"   Track: {track_id}, Class: {class_id}")
        
        # ì •í™•í•œ ë¦¬ì‚¬ì´ì¦ˆ
        mask = cv2.resize(
            mask.astype(np.float32), 
            (image.shape[1], image.shape[0]),  # (width, height) ìˆœì„œ ì£¼ì˜
            interpolation=cv2.INTER_LINEAR
        )
        print(f"   Resized: {mask.shape[:2]}")
    
    # boolean ë§ˆìŠ¤í¬ë¡œ ë³€í™˜ (threshold ì¡°ì •)
    mask = mask > 0.3  # thresholdë¥¼ ë‚®ì¶°ì„œ ë” ë§ì€ ì˜ì—­ í¬í•¨
    
    # ë§ˆìŠ¤í¬ ì˜ì—­ì´ ìˆëŠ”ì§€ í™•ì¸
    mask_area = np.sum(mask)
    if mask_area == 0:
        print(f"âš ï¸ ë¹ˆ ë§ˆìŠ¤í¬: track_id={track_id}")
        return image
    
    # Supervision ë°©ì‹: colored_mask ìƒì„± í›„ addWeighted ì‚¬ìš©
    colored_mask = np.array(image, copy=True, dtype=np.uint8)
    colored_mask[mask] = color  # ë§ˆìŠ¤í¬ ì˜ì—­ì—ë§Œ ìƒ‰ìƒ ì ìš©
    
    # addWeightedë¡œ ë¸”ë Œë”© (ì›ë³¸ ì´ë¯¸ì§€ì— ì§ì ‘ ì ìš©)
    cv2.addWeighted(colored_mask, opacity, image, 1 - opacity, 0, dst=image)
    
    return image

def draw_label(image, box, track_id, class_name, confidence, color_palette):
    """ë¼ë²¨ ê·¸ë¦¬ê¸°"""
    x1, y1, x2, y2 = map(int, box)
    color = color_palette.by_idx(track_id).as_bgr()
    
    # ë¼ë²¨ í…ìŠ¤íŠ¸
    label = f"{class_name} {track_id} {confidence:.2f}"
    
    # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.6
    thickness = 2
    (text_w, text_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)
    
    # ë¼ë²¨ ë°°ê²½ ê·¸ë¦¬ê¸°
    cv2.rectangle(image, (x1, y1 - text_h - baseline - 10), 
                  (x1 + text_w, y1), color, -1)
    
    # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
    cv2.putText(image, label, (x1, y1 - baseline - 5), 
                font, font_scale, (255, 255, 255), thickness)
    
    return image

def draw_objects_overlay(image, objects, color_palette):
    """ëª¨ë“  ê°œì²´ì˜ overlay ê·¸ë¦¬ê¸° (Supervision ë°©ì‹: í° ê°ì²´ë¶€í„° ì‘ì€ ê°ì²´ ìˆœ)"""
    # area ê³„ì‚°í•˜ì—¬ í° ê²ƒë¶€í„° ì •ë ¬ (Supervisionê³¼ ë™ì¼)
    valid_objects = [obj for obj in objects if obj.track_id is not None]
    
    if not valid_objects:
        return image
    
    # ë°•ìŠ¤ area ê³„ì‚°
    areas = []
    for obj in valid_objects:
        x1, y1, x2, y2 = obj.box
        area = (x2 - x1) * (y2 - y1)
        areas.append(area)
    
    # area ê¸°ì¤€ìœ¼ë¡œ í° ê²ƒë¶€í„° ì •ë ¬ (flipìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ)
    sorted_indices = np.flip(np.argsort(areas))
    
    # ë§ˆìŠ¤í¬ë¶€í„° ë¨¼ì € ê·¸ë¦¬ê¸° (supervision ë°©ì‹)
    for idx in sorted_indices:
        obj = valid_objects[idx]
        # ë§ˆìŠ¤í¬ ê·¸ë¦¬ê¸°
        image = draw_mask(image, obj.mask, obj.track_id, obj.class_id, color_palette)
    
    # ê·¸ ë‹¤ìŒ ë°•ìŠ¤ì™€ ë¼ë²¨ ê·¸ë¦¬ê¸° (ì›ë˜ ìˆœì„œëŒ€ë¡œ)
    for obj in valid_objects:
        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        image = draw_box(image, obj.box, obj.track_id, obj.class_id, color_palette)
        
        # ë¼ë²¨ ê·¸ë¦¬ê¸°
        image = draw_label(image, obj.box, obj.track_id, obj.class_name, 
                          obj.confidence, color_palette)
    
    return image


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def preprocess_image(image, args):
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    # ì´ë¯¸ì§€ë¥¼ float32ë¡œ ë³€í™˜
    img = image.astype(np.float32)
    
    # ì„ ëª…ë„ í–¥ìƒ
    if args.sharpen > 0:
        kernel = np.array([[-1,-1,-1],
                          [-1, 9,-1],
                          [-1,-1,-1]]) * args.sharpen
        img = cv2.filter2D(img, -1, kernel)
    
    # ëŒ€ë¹„ ì¡°ì •
    if args.contrast != 1.0:
        img = cv2.convertScaleAbs(img, alpha=args.contrast, beta=0)
    
    # ë°ê¸° ì¡°ì •
    if args.brightness != 0:
        img = cv2.convertScaleAbs(img, alpha=1.0, beta=args.brightness)
    
    # ë…¸ì´ì¦ˆ ì œê±°
    if args.denoise > 0:
        img = cv2.fastNlMeansDenoisingColored(img, None, 
                                             h=args.denoise*10, 
                                             hColor=args.denoise*10, 
                                             templateWindowSize=7, 
                                             searchWindowSize=21)
    
    # ê°’ ë²”ìœ„ë¥¼ 0-255ë¡œ ì œí•œ
    img = np.clip(img, 0, 255).astype(np.uint8)
    return img

def main() -> None:
    args = parse_args()
    # Hybrid VP íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’ 10, 10)
    KEY_INT = getattr(args, 'key_int', 10)
    HOLD_FR = getattr(args, 'hold', 10)
    drift_max = HOLD_FR  # ì—°ì† ê²€ì¶œ ì‹¤íŒ¨ì‹œ ë¦¬ì…‹
    drift_count = 0

    # ì…ë ¥ íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ìë¥¼ ì œì™¸í•œ ê¸°ë³¸ ì´ë¦„ ì¶”ì¶œ
    base_name = os.path.splitext(os.path.basename(args.source))[0]
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output is None:
        output_dir = os.path.join(os.path.dirname(args.source), base_name)
    else:
        output_dir = args.output
    
    # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ìˆë‹¤ë©´ ì‚­ì œ
    if os.path.exists(output_dir):
        import shutil
        print(f"ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘: {output_dir}")
        shutil.rmtree(output_dir)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    print(f"ìƒˆë¡œìš´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    
    # ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ ì„¤ì •
    output_video = os.path.join(output_dir, f"{base_name}_output.mp4")

    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    log_file = None
    if args.log_detections:
        log_path = os.path.join(output_dir, "label.txt")
        log_file = open(log_path, "w", encoding="utf-8")
        log_file.write("Frame_Time,Class,TrackID,Class_Count\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ I/O setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    cap = cv2.VideoCapture(args.source)
    if not cap.isOpened():
        raise FileNotFoundError(f"Cannot open video {args.source}")

    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps, total_frames = cap.get(cv2.CAP_PROP_FPS), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Select frames to process
    frames_to_process = range(total_frames)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Model & palette â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    model = YOLOE(args.checkpoint)
    model.eval()
    model.to(args.device)
    model.set_classes(args.names, model.get_text_pe(args.names))

    # VP ëª¨ë¸ ë³„ë„ ë¡œë“œ
    model_vp = YOLOE(args.checkpoint)
    model_vp.eval()
    model_vp.to(args.device)
    model_vp.set_classes(args.names, model_vp.get_text_pe(args.names))

    person_class_id = args.names.index("fish") if "fish" in args.names else 0
    palette = sv.ColorPalette.DEFAULT
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    tracker = BoostTrack(
        video_name=args.source,
        det_thresh=args.track_det_thresh,
        iou_threshold=args.track_iou_thresh
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Counting line (pixel) â”€â”€â”€â”€â”€ #
    if args.line_start is not None and args.line_end is not None:
        p1 = (int(args.line_end[0]   * width),  int(args.line_end[1]   * height))
        p2 = (int(args.line_start[0] * width),  int(args.line_start[1] * height))
        
        # ì„ ë¶„ ë²¡í„° ë° ê¸¸ì´Â² ê³„ì‚°
        seg_dx, seg_dy = p2[0] - p1[0], p2[1] - p1[1]
        seg_len2 = seg_dx * seg_dx + seg_dy * seg_dy or 1
    else:
        p1 = p2 = None
        seg_dx = seg_dy = seg_len2 = 0

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Runtime state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    track_history = defaultdict(list)
    track_side   = {}
    track_color  = {}
    forward_cnt = backward_cnt = 0

    # íŠ¸ë˜í‚¹ ë¼ì¸ ê´€ë ¨ ë³€ìˆ˜
    max_track_history = 30  # ìµœëŒ€ íŠ¸ë˜í‚¹ íˆìŠ¤í† ë¦¬ ê¸¸ì´
    line_thickness = 2     # ë¼ì¸ ë‘ê»˜
    line_opacity = 0.5     # ë¼ì¸ íˆ¬ëª…ë„

    # --- í”„ë¡¬í”„íŠ¸ ë°˜ë³µ ë¡œì§ìš© ìƒíƒœ ë³€ìˆ˜ --- #
    prev_prompt = None
    prompt_frame = None
    last_vp_frame = None
    last_vp_results = None
    
    # VPE moving average ìƒíƒœ ë³€ìˆ˜
    vpe_avg = None
    vpe_alpha = 0.9  # moving average ê³„ìˆ˜ (0.9 = ì´ì „ ê°’ì˜ 90% + í˜„ì¬ ê°’ì˜ 10%)

    # Heat-map ëˆ„ì  ë²„í¼
    heatmap = np.zeros((height, width), dtype=np.float32)

    # ë¼ì¸ ê¹œë°•ì„ ìƒíƒœ
    line_flash = False

    # 1ì´ˆë§ˆë‹¤ ê°±ì‹ ë˜ëŠ” ê°’ë“¤
    last_update_time = 0
    current_occupancy = 0
    current_congestion = 0
    frame_count = 0

    # ì¤‘ê°„ ì €ì¥ ê´€ë ¨ ë³€ìˆ˜
    last_save_frame = 0
    log_buffer = []  # ë¡œê·¸ ë²„í¼

    # ë¹„ë””ì˜¤ ì¶œë ¥ ì„¤ì •
    out = cv2.VideoWriter(output_video,
                        cv2.VideoWriter_fourcc(*'mp4v'),
                        fps, (width, height))

    pbar = tqdm(total=len(frames_to_process), desc="Processing")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Frame loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
    for frame_idx in frames_to_process:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ok, frame_bgr = cap.read()
        if not ok:
            break

        frame_count += 1
        current_time = frame_idx / fps  # ì‹¤ì œ ì˜ìƒì˜ ì‹œê°„ ê³„ì‚°

        # í˜„ì¬ í”„ë ˆì„ ì‹œê°„ ê³„ì‚° (ì‹œê°„:ë¶„:ì´ˆ:ë°€ë¦¬ì´ˆ)
        hours = int(current_time // 3600)
        minutes = int((current_time % 3600) // 60)
        seconds = int(current_time % 60)
        milliseconds = int((current_time % 1) * 1000)
        time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"

        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì ìš©
        frame_rgb = preprocess_image(frame_rgb, args)
        
        image_pil = Image.fromarray(frame_rgb)

        # â”€â”€ Detection (track ëŒ€ì‹  predictë§Œ ì‚¬ìš©) â”€â”€ #
        if prev_prompt is not None and prompt_frame is not None and args.cross_vp:
            # ì´ì „ í”„ë¡¬í”„íŠ¸ë¡œ Cross-Image VP
            model_vp.predictor = YOLOEVPSegPredictor
            model_vp.predictor = None
            model_vp.predict(
                source=prompt_frame,
                imgsz=args.image_size,
                conf=args.vp_thresh,
                iou=args.iou_thresh,
                return_vpe=True,
                prompts=prev_prompt,
                predictor=YOLOEVPSegPredictor
            )
            if hasattr(model_vp, 'predictor') and hasattr(model_vp.predictor, 'vpe'):
                current_vpe = model_vp.predictor.vpe
                if vpe_avg is None:
                    vpe_avg = current_vpe
                else:
                    vpe_avg = vpe_alpha * vpe_avg + (1 - vpe_alpha) * current_vpe
                model_vp.set_classes(args.names, vpe_avg)
                model_vp.predictor = None
            else:
                print("[DEBUG] VPE ìƒì„± ì‹¤íŒ¨!")

            results = model_vp.predict(
                source=image_pil,
                imgsz=args.image_size,
                conf=args.conf_thresh,
                iou=args.iou_thresh,
                verbose=False
            )
            result = results[0] if isinstance(results, list) else results
        else:
            results = model.predict(
                source=image_pil,
                imgsz=args.image_size,
                conf=0.05,
                iou=args.iou_thresh,
                verbose=False
            )
            result = results[0] if isinstance(results, list) else results

        # 1ì°¨ í”„ë¡¬í”„íŠ¸ ê°±ì‹ 
        if len(result.boxes) > 0:
            confidences = result.boxes.conf.cpu().numpy()
            boxes = result.boxes.xyxy.cpu().numpy()
            high_conf_mask = confidences >= args.vp_thresh
            if np.any(high_conf_mask):
                # ê° boxì— í•´ë‹¹í•˜ëŠ” class ID ì‚¬ìš©
                class_ids = result.boxes.cls.cpu().numpy()[high_conf_mask]
                prev_prompt = {
                    "bboxes": [boxes[high_conf_mask]],
                    "cls": [class_ids]
                }
                
                # ëª¨ë“  í´ë˜ìŠ¤ê°€ í¬í•¨ë˜ë„ë¡ ì¶”ê°€
                for cls_idx in range(len(args.names)):
                    if cls_idx not in class_ids:
                        prev_prompt["bboxes"][0] = np.append(prev_prompt["bboxes"][0], [[0, 0, 0, 0]], axis=0)
                        prev_prompt["cls"][0] = np.append(prev_prompt["cls"][0], [cls_idx])
                
                prompt_frame = image_pil.copy()
            else:
                prev_prompt = None
                prompt_frame = None
        else:
            prev_prompt = None
            prompt_frame = None

        # â”€â”€ ObjectMeta ì¤‘ì‹¬ ì²˜ë¦¬ ì‹œì‘ â”€â”€ #
        # 1. Model ê²°ê³¼ë¥¼ ObjectMeta ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        detected_objects = convert_results_to_objects(result, args.names)
        
        # 2. ìƒˆë¡œìš´ ObjectMeta ê¸°ë°˜ íŠ¸ë˜ì»¤ ì‚¬ìš©
        tracked_objects = tracker.update(detected_objects, result.orig_img, "None")
        
        # 3. ê¸°ì¡´ ë³€ìˆ˜ë“¤ì„ ObjectMetaì—ì„œ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ í˜¸í™˜ì„±ì„ ìœ„í•´)
        if tracked_objects:
            # tracked_objectsì—ì„œ xywh í˜•íƒœë¡œ ë³€í™˜ (ê¸°ì¡´ ë¡œì§ í˜¸í™˜)
            boxes_xywh = []
            for obj in tracked_objects:
                x1, y1, x2, y2 = obj.box  # xyxy
                cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
                w, h = x2 - x1, y2 - y1
                boxes_xywh.append([cx, cy, w, h])
            boxes_xywh = np.array(boxes_xywh)
            
            class_ids = np.array([obj.class_id for obj in tracked_objects])
            track_ids = np.array([obj.track_id for obj in tracked_objects])
        else:
            boxes_xywh = np.empty((0, 4))
            class_ids = np.empty(0, int)
            track_ids = np.empty(0, int)

        # ë¡œê·¸ íŒŒì¼ì— ê²€ì¶œ ì •ë³´ ê¸°ë¡
        if log_file is not None:
            class_counts = defaultdict(int)
            for cid in class_ids:
                class_counts[cid] += 1

            # í´ë˜ìŠ¤ë³„ ê²€ì¶œ ìˆ«ì ì¶œë ¥
            print(f"Frame {frame_idx} - Class counts:")
            for cid, count in class_counts.items():
                print(f"  {args.names[cid]}: {count}")

            for cid, tid in zip(class_ids, track_ids):
                class_name = args.names[cid]
                class_count = class_counts[cid]
                log_entry = f"{time_str},{class_name},{tid},{class_count}\n"
                if log_file is not None:
                    log_buffer.append(log_entry)
                    if len(log_buffer) >= 100:
                        log_file.writelines(log_buffer)
                        log_file.flush()
                        log_buffer.clear()

        # â”€â”€ ìƒˆë¡œìš´ ObjectMeta ê¸°ë°˜ Overlay ì‹œìŠ¤í…œ â”€â”€ #
        annotated = frame_rgb.copy()
        if tracked_objects:
            # ì§ì ‘ êµ¬í˜„í•œ overlay í•¨ìˆ˜ ì‚¬ìš©
            annotated = draw_objects_overlay(annotated, tracked_objects, palette)

        # â”€â”€ People occupancy - 1ì´ˆë§ˆë‹¤ ê°±ì‹ 
        raw_occupancy = int(np.sum(class_ids == person_class_id))

        if current_time - last_update_time >= 1.0:
            last_update_time = current_time
            current_occupancy = raw_occupancy
            current_congestion = int(min(current_occupancy / max(args.max_people, 1), 1.0) * 100)

        line_crossed_this_frame = False

        # â”€â”€ ObjectMeta ê¸°ë°˜ íˆíŠ¸ë§µ ë° ë¼ì¸ í¬ë¡œì‹± ì²˜ë¦¬ â”€â”€ #
        for obj in tracked_objects:
            if obj.track_id == -1 or obj.class_id != person_class_id:
                continue

            # ë°•ìŠ¤ ì¤‘ì‹¬ì  ê³„ì‚° (xyxy â†’ center)
            x1, y1, x2, y2 = obj.box
            cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
            ix, iy = int(cx), int(cy)
            
            # íˆíŠ¸ë§µ ì—…ë°ì´íŠ¸
            if 0 <= iy < height and 0 <= ix < width:
                radius = 10
                for dy in range(-radius, radius + 1):
                    for dx in range(-radius, radius + 1):
                        if dx*dx + dy*dy <= radius*radius:
                            ny, nx = iy + dy, ix + dx
                            if 0 <= ny < height and 0 <= nx < width:
                                intensity = 1.0 * (1.0 - ((dx*dx + dy*dy) / (radius*radius)))
                                heatmap[ny, nx] += intensity

            # íŠ¸ë˜í‚¹ ì»¬ëŸ¬ ì„¤ì •
            if obj.track_id not in track_color:
                track_color[obj.track_id] = palette.by_idx(obj.track_id).as_bgr()

            # ë¼ì¸ í¬ë¡œì‹± ì²´í¬
            if p1 is not None and p2 is not None:
                side_now = point_side((cx, cy), p1, p2)
                t = ((cx - p1[0]) * seg_dx + (cy - p1[1]) * seg_dy) / seg_len2
                inside = 0.0 <= t <= 1.0

                side_prev = track_side.get(obj.track_id, side_now)
                if inside and (side_prev * side_now < 0):
                    line_crossed_this_frame = True
                    if side_prev < 0 < side_now:
                        forward_cnt += 1
                    elif side_prev > 0 > side_now:
                        backward_cnt += 1
                if inside and side_now != 0:
                    track_side[obj.track_id] = side_now

        if p1 is not None and p2 is not None:
            if line_crossed_this_frame:
                cv2.line(annotated, p1, p2, (255, 0, 0), 8)
            else:
                cv2.line(annotated, p1, p2, (255, 255, 255), 8)

            mid_x, mid_y = (p1[0] + p2[0]) // 2, (p1[1] + p2[1]) // 2
            dx, dy = p2[0] - p1[0], p2[1] - p1[1]
            perp_x, perp_y = -dy, dx
            length = 50
            mag = (perp_x**2 + perp_y**2) ** 0.5 or 1
            perp_x, perp_y = int(perp_x / mag * length), int(perp_y / mag * length)
            arrow_start, arrow_end = (mid_x, mid_y), (mid_x + perp_x, mid_y + perp_y)
            cv2.arrowedLine(annotated, arrow_start, arrow_end, (0, 255, 0), 8, tipLength=0.6)

        overlay = [
            f"[1] Congestion : {current_congestion:3d} %",
            f"[2] Crossing   : forward {forward_cnt} | backward {backward_cnt}",
            f"[3] Occupancy  : {current_occupancy}",
            f"[4] Time      : {time_str}"
        ]
        x0, y0, dy = 30, 60, 50
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1
        font_thickness = 2
        padding = 10
        max_width = 0
        for txt in overlay:
            (tw, th), _ = cv2.getTextSize(txt, font, font_scale, font_thickness)
            max_width = max(max_width, tw)
        total_height = th * len(overlay) + dy * (len(overlay) - 1)
        rect_x1, rect_y1 = x0 - padding, y0 - th - padding
        rect_x2, rect_y2 = x0 + max_width + padding, y0 + (len(overlay) - 1) * dy + padding
        overlay_rect = annotated.copy()
        cv2.rectangle(overlay_rect, (rect_x1, rect_y1), (rect_x2, rect_y2), (0, 0, 0), -1)
        annotated = cv2.addWeighted(overlay_rect, 0.4, annotated, 0.6, 0)
        for i, txt in enumerate(overlay):
            cv2.putText(annotated, txt, (x0, y0 + i * dy),
                        font, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)

        blur = cv2.GaussianBlur(heatmap, (0, 0), sigmaX=15, sigmaY=15)
        norm = cv2.normalize(blur, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        color_hm = cv2.applyColorMap(norm, cv2.COLORMAP_JET)
        color_hm = cv2.cvtColor(color_hm, cv2.COLOR_BGR2RGB)

        mini_w = 400
        mini_h = int(height * mini_w / width)
        mini_map = cv2.resize(color_hm, (mini_w, mini_h))

        margin = 20
        x_start = width - mini_w - margin
        y_start = margin
        roi = annotated[y_start:y_start + mini_h, x_start:x_start + mini_w]
        blended = cv2.addWeighted(mini_map, 0.6, roi, 0.4, 0)
        annotated[y_start:y_start + mini_h, x_start:x_start + mini_w] = blended

        if frame_idx - last_save_frame >= args.save_interval:
            snapshot_path = os.path.join(output_dir, f"intermediate_{frame_idx:06d}_{time_str.replace(':', '-')}.jpg")
            cv2.imwrite(snapshot_path, cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))
            last_save_frame = frame_idx
            print(f"\nì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {snapshot_path}")

        out.write(cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))
        pbar.update(1)

    pbar.close()
    cap.release()
    if log_file is not None:
        if log_buffer:
            log_file.writelines(log_buffer)
        log_file.close()
    out.release()
    print(f"âœ” ì™„ë£Œ. ì €ì¥ ìœ„ì¹˜: {output_dir}")


if __name__ == "__main__":
    main() 
